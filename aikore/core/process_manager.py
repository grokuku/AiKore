import subprocess
import os
import stat
import socket
import re
import textwrap
import threading
import time
import requests
import signal
import psutil
import pty
import fcntl  # NEW: For terminal resizing
import termios # NEW: For terminal resizing
import struct  # NEW: For terminal resizing
from pathlib import Path
from subprocess import PIPE, STDOUT
from sqlalchemy.orm import Session

# We will need access to the DB models and the session factory
from aikore.database import models, crud
from aikore.database.session import SessionLocal

# --- CONSTANTS ---
INSTANCES_DIR = "/config/instances"
OUTPUTS_DIR = "/config/outputs"
BLUEPRINTS_DIR = "/opt/sd-install/blueprints"
# CORRECTED: Define both NGINX paths clearly
NGINX_CONF_D = "/etc/nginx/conf.d"
NGINX_LOCATIONS_D = "/etc/nginx/locations.d"
NGINX_RELOAD_FLAG = Path("/run/aikore/nginx_reload.flag")

# Timeout in seconds before an instance is marked as 'stalled'
STALLED_TIMEOUT = 180
# How often the monitor thread checks the web port
MONITOR_POLL_INTERVAL = 2

# --- GLOBAL STATE ---
# In-memory dictionary to keep track of running processes and their monitor threads
# Structure: { instance_id: {
#    "process": Popen_object, "monitor_thread": Thread_object,
#    "internal_port": int, "activation_process": Popen_object | None
# }}
running_instances = {}

# --- HELPER FUNCTIONS ---

def _slugify(value: str) -> str:
    value = value.lower()
    value = re.sub(r'[^\w\s-]', '', value)
    value = re.sub(r'[\s_-]+', '-', value).strip('-')
    return value

def _find_free_port() -> int:
    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
        s.bind(('', 0))
        return s.getsockname()[1]

def _find_free_display() -> int:
    display = 10
    while os.path.exists(f"/tmp/.X11-unix/X{display}"):
        display += 1
    return display

def _reload_nginx():
    try:
        NGINX_RELOAD_FLAG.touch()
    except Exception as e:
        print(f"[ERROR] Failed to request NGINX reload: {e}")

def _cleanup_instance_files(instance_slug: str):
    """Cleans up NGINX conf and other temp files for an instance."""
    # This function is now only for persistent mode configs
    nginx_conf_path = os.path.join(NGINX_LOCATIONS_D, f"{instance_slug}.conf")
    if os.path.exists(nginx_conf_path):
        os.remove(nginx_conf_path)
        # A reload is required after removing a conf file for persistent mode
        _reload_nginx()
    
    # Cleanup Firefox profile if it exists
    firefox_profile_dir = f"/tmp/firefox-profiles/{instance_slug}"
    if os.path.isdir(firefox_profile_dir):
        import shutil
        shutil.rmtree(firefox_profile_dir, ignore_errors=True)

# --- RE-ARCHITECTED: COMFYUI PROXY MANAGEMENT ---

def update_comfyui_proxy(db: Session):
    """
    Creates/removes the two NGINX files (upstream and locations) for the active ComfyUI slot.
    This solves the "upstream directive not allowed" error.
    It now intelligently finds the internal port of the running instance.
    """
    upstream_conf_path = os.path.join(NGINX_CONF_D, "aikore-comfyui-upstream.conf")
    locations_conf_path = os.path.join(NGINX_LOCATIONS_D, "00-comfyui-active-slot.conf")
    
    active_instance = crud.get_active_comfyui_slot(db)
    internal_port = None

    if active_instance and active_instance.id in running_instances:
        internal_port = running_instances[active_instance.id].get("internal_port")

    if active_instance and active_instance.status == "started" and internal_port:
        print(f"[Manager] Activating ComfyUI proxy for '{active_instance.name}' on internal port {internal_port}")
        
        # File 1: The upstream configuration (goes in conf.d)
        upstream_content = textwrap.dedent(f"""\
            # This file is dynamically generated by AiKore. Do not edit.
            # Defines the upstream server for the active ComfyUI instance.
            upstream comfyui_active_slot {{
                server 127.0.0.1:{internal_port};
            }}
        """)
        with open(upstream_conf_path, 'w') as f:
            f.write(upstream_content)

        # File 2: The locations configuration (goes in locations.d)
        locations_content = textwrap.dedent(f"""\
            # This file is dynamically generated by AiKore. Do not edit.
            # Active ComfyUI instance: {active_instance.name} (ID: {active_instance.id})

            # 1. Main UI & API Location: Handles all requests prefixed with /comfyui/.
            location /comfyui/ {{
                # This intelligent rewrite handles all of ComfyUI's inconsistencies:
                # - /comfyui/api/history -> /history
                # - /comfyui/ws         -> /ws
                # - /comfyui/           -> /
                rewrite ^/comfyui(?:/api)?/(.*)$ /$1 break;
                
                proxy_pass http://comfyui_active_slot;
                proxy_http_version 1.1;
                proxy_set_header Upgrade $http_upgrade;
                proxy_set_header Connection "upgrade";
                proxy_set_header Host $host;
                proxy_set_header X-Real-IP $remote_addr;
                proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
                proxy_set_header X-Forwarded-Proto $scheme;
                proxy_buffering off;
            }}
            
            # 2. Outlier Location: Handles the "Save Workflow" call which is not prefixed.
            location /workflows/ {{
                proxy_pass http://comfyui_active_slot;
                proxy_set_header Host $host;
                proxy_set_header X-Real-IP $remote_addr;
                proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
                proxy_set_header X-Forwarded-Proto $scheme;
            }}
        """)
        with open(locations_conf_path, 'w') as f:
            f.write(locations_content)

    else:
        # If no active instance or it's not started, remove both conf files.
        print("[Manager] No active & started ComfyUI slot. Deactivating proxy configuration.")
        if os.path.exists(upstream_conf_path):
            os.remove(upstream_conf_path)
        if os.path.exists(locations_conf_path):
            os.remove(locations_conf_path)
    
    # Always trigger a reload to apply the changes (creation or deletion).
    _reload_nginx()

# --- NEW: TERMINAL MANAGEMENT ---

def parse_blueprint_metadata(blueprint_filename: str) -> dict:
    """
    Parses the metadata block from a blueprint shell script.
    """
    metadata = {}
    blueprint_path = os.path.join(BLUEPRINTS_DIR, blueprint_filename)
    if not os.path.exists(blueprint_path):
        return metadata

    with open(blueprint_path, 'r') as f:
        in_metadata_block = False
        for line in f:
            if "### AIKORE-METADATA-START ###" in line:
                in_metadata_block = True
                continue
            if "### AIKORE-METADATA-END ###" in line:
                break
            if in_metadata_block:
                line = line.strip()
                if line.startswith('#') and '=' in line:
                    # Format is '# aikore.key = value'
                    parts = line[1:].strip().split('=', 1)
                    if len(parts) == 2 and parts[0].strip().startswith('aikore.'):
                        key = parts[0].strip().replace('aikore.', '', 1)
                        value = parts[1].strip()
                        metadata[key] = value
    return metadata


def start_terminal_process(instance: models.Instance):
    """
    Spawns a shell process inside a pseudo-terminal (PTY) for a given instance.
    """
    instance_conf_dir = os.path.join(INSTANCES_DIR, instance.name)
    metadata = parse_blueprint_metadata(instance.base_blueprint)
    
    venv_type = metadata.get('venv_type')
    venv_path = metadata.get('venv_path')

    command = ['bin/bash']
    env = os.environ.copy()

    if venv_type and venv_path:
        full_venv_path = os.path.join(instance_conf_dir, venv_path)
        if venv_type == 'conda':
            # This launches a shell, sources the activate script for the specific env,
            # then 'exec' replaces that shell with a new one that inherits the environment.
            conda_activate_cmd = f"source /home/abc/miniconda3/bin/activate {full_venv_path} && exec /bin/bash"
            command = ['bin/bash', '-c', conda_activate_cmd]
        elif venv_type == 'python':
            # For standard python venv, --rcfile is the correct approach
            activate_script = os.path.join(full_venv_path, 'bin', 'activate')
            command = ['bin/bash', '--rcfile', activate_script]

    # Fork a process and connect the child's controlling terminal to a new PTY
    pid, master_fd = pty.fork()

    if pid == 0:  # Child process
        # Set the working directory for the new shell
        os.chdir(instance_conf_dir)
        # Execute the shell command
        os.execve(command[0], command, env)
    else:  # Parent process
        return pid, master_fd

# NEW: Function to handle resizing the PTY
def resize_terminal_process(master_fd: int, rows: int, cols: int):
    """
    Resizes the pseudo-terminal window size.
    """
    try:
        # Pack the new window size into a struct
        winsize = struct.pack('HHHH', rows, cols, 0, 0)
        # Set the new window size using a system call
        fcntl.ioctl(master_fd, termios.TIOCSWINSZ, winsize)
    except Exception as e:
        print(f"[ERROR] Failed to resize terminal: {e}")


# --- CORE MONITORING LOGIC ---

def monitor_instance_thread(instance_id: int, pid: int, port: int, vnc_display: int | None, instance_slug: str):
    """
    Runs in a background thread to monitor an instance's web server.
    Updates the instance status and launches Firefox when ready.
    """
    start_time = time.time()
    
    while psutil.pid_exists(pid):
        # Check if the web application is accessible
        try:
            response = requests.get(f"http://127.0.0.1:{port}", timeout=2)
            # We consider any successful HTTP response as the service being ready
            if response.status_code < 500:
                print(f"[Monitor-{instance_id}] Instance is RUNNING on port {port}.")
                with SessionLocal() as db:
                    instance = db.query(models.Instance).filter(models.Instance.id == instance_id).first()
                    if instance:
                        instance.status = "started"
                        db.commit()

                        # If this is a ComfyUI instance that is the active slot, update the proxy now
                        metadata = parse_blueprint_metadata(instance.base_blueprint)
                        if metadata.get('app_id') == 'comfyui' and not instance.persistent_mode and instance.is_comfyui_active_slot:
                            update_comfyui_proxy(db)

                # If in VNC mode, launch Firefox now
                if vnc_display is not None:
                    print(f"[Monitor-{instance_id}] VNC mode detected. Launching Firefox on display :{vnc_display}.")
                    firefox_profile_dir = f"/tmp/firefox-profiles/{instance_slug}"
                    os.makedirs(firefox_profile_dir, exist_ok=True)
                    
                    ff_env = os.environ.copy()
                    ff_env["DISPLAY"] = f":{vnc_display}"
                    
                    target_url = f'http://127.0.0.1:{port}'
                    
                    # Use the explicit '-url' argument for better reliability
                    subprocess.Popen(
                        ['usr/bin/firefox', '--profile', firefox_profile_dir, '--kiosk', '-url', target_url],
                        env=ff_env
                    )
                break # Exit the monitoring loop on success
        
        except requests.exceptions.ConnectionError:
            # Application is not yet ready, check for timeout
            elapsed_time = time.time() - start_time
            if elapsed_time > STALLED_TIMEOUT:
                with SessionLocal() as db:
                    # Update to 'stalled' only if it's currently 'starting'
                    instance = db.query(models.Instance).filter(models.Instance.id == instance_id).first()
                    if instance and instance.status == "starting":
                        instance.status = "stalled"
                        db.commit()
                        print(f"[Monitor-{instance_id}] Instance has been starting for >{STALLED_TIMEOUT}s. Marked as STALLED.")
            
            time.sleep(MONITOR_POLL_INTERVAL)
        
        except Exception as e:
            print(f"[Monitor-{instance_id}] An unexpected error occurred: {e}")
            time.sleep(MONITOR_POLL_INTERVAL)
    
    print(f"[Monitor-{instance_id}] Process with PID {pid} no longer exists. Monitor thread exiting.")

# --- PROCESS MANAGEMENT INTERFACE ---

def activate_instance_port(instance: models.Instance):
    """
    Activates the public port for a given running instance using socat.
    """
    if instance.id not in running_instances or instance.persistent_mode:
        return

    instance_state = running_instances[instance.id]
    if instance_state.get("activation_process"):
        print(f"[Manager] Port for instance {instance.id} is already active.")
        return
    
    public_port = instance.port
    internal_port = instance_state.get("internal_port")
    if not public_port or not internal_port:
        raise ValueError("Instance is missing public or internal port information.")
    
    cmd = [
        'socat', 
        f'TCP-LISTEN:{public_port},fork,reuseaddr', 
        f'TCP:127.0.0.1:{internal_port}'
    ]
    
    try:
        # We don't need stdout/stderr for socat, it can run detached.
        activation_process = subprocess.Popen(cmd)
        instance_state["activation_process"] = activation_process
        print(f"[Manager] Activated public port {public_port} for instance '{instance.name}' (socat PID: {activation_process.pid}).")
    except FileNotFoundError:
        err_msg = "The 'socat' command was not found. Please ensure it is installed in the Docker container."
        print(f"[ERROR] {err_msg}")
        raise RuntimeError(err_msg)
    except Exception as e:
        err_msg = f"An unexpected error occurred while starting socat: {e}"
        print(f"[ERROR] {err_msg}")
        raise RuntimeError(err_msg)

def deactivate_instance_port(instance_id: int):
    """
    Deactivates the public port for a given instance by stopping its socat process.
    """
    if instance_id not in running_instances:
        return
        
    instance_state = running_instances[instance_id]
    activation_process = instance_state.get("activation_process")
    
    if activation_process:
        print(f"[Manager] Deactivating public port for instance {instance_id} (socat PID: {activation_process.pid})...")
        try:
            activation_process.terminate()
            activation_process.wait(timeout=5)
        except subprocess.TimeoutExpired:
            activation_process.kill()
        except Exception as e:
            print(f"[Manager] Error stopping socat process: {e}")
        finally:
            instance_state["activation_process"] = None

def start_instance_process(db: Session, instance: models.Instance):
    """
    Starts the instance process and its associated monitoring thread. Does NOT activate public port.
    """
    if instance.id in running_instances:
        raise Exception(f"Instance {instance.id} is already running.")

    instance_conf_dir = os.path.join(INSTANCES_DIR, instance.name)
    instance_output_dir = os.path.join(OUTPUTS_DIR, instance.name)
    instance_slug = _slugify(instance.name)

    os.makedirs(instance_conf_dir, exist_ok=True)
    os.makedirs(instance_output_dir, exist_ok=True)
    os.makedirs(NGINX_CONF_D, exist_ok=True)
    os.makedirs(NGINX_LOCATIONS_D, exist_ok=True)

    global_tmp_dir = "/config/tmp"
    os.makedirs(global_tmp_dir, exist_ok=True)

    dest_script_path = os.path.join(instance_conf_dir, "launch.sh")
    source_script_path = os.path.join(BLUEPRINTS_DIR, instance.base_blueprint)
    try:
        with open(source_script_path, 'r') as src, open(dest_script_path, 'w') as dest:
            dest.write(src.read())
    except FileNotFoundError:
        raise Exception(f"Blueprint file not found at {source_script_path}")
    os.chmod(dest_script_path, os.stat(dest_script_path).st_mode | stat.S_IEXEC)

    internal_app_port = _find_free_port()

    env = os.environ.copy()
    env["TMPDIR"] = global_tmp_dir
    if instance.gpu_ids:
        env["CUDA_VISIBLE_DEVICES"] = instance.gpu_ids
    
    env["WEBUI_PORT"] = str(internal_app_port)
    env["INSTANCE_NAME"] = instance.name
    env["INSTANCE_CONF_DIR"] = instance_conf_dir
    env["INSTANCE_OUTPUT_DIR"] = instance_output_dir
    env["BLUEPRINT_ID"] = os.path.splitext(instance.base_blueprint)[0]
    
    main_cmd = []
    
    if instance.persistent_mode:
        # Persistent mode logic remains mostly unchanged
        if instance.vnc_display is None or instance.vnc_port is None:
            instance.status = "stopped"
            db.commit()
            raise ValueError("Instance in persistent mode is missing VNC port/display values.")

        vnc_launcher_path = os.path.join(instance_conf_dir, f"vnc_launcher_{instance_slug}.sh")
        # ... (VNC script content is unchanged)
        vnc_launcher_content = f"""
        #!/bin/bash
        cleanup() {{ kill 0; }}
        trap cleanup SIGTERM SIGINT
        /usr/bin/Xvnc :{instance.vnc_display} -rfbport {5900 + instance.vnc_display} -SecurityTypes None -MaxIdleTime 0 &
        XVNC_PID=$!
        VNC_PORT=$((5900 + {instance.vnc_display}))
        echo "Waiting for VNC server on port $VNC_PORT..."
        count=0
        while ! (exec 3<>/dev/tcp/127.0.0.1/$VNC_PORT) 2>/dev/null; do
            if ! kill -0 $XVNC_PID 2>/dev/null; then echo "[ERROR] Xvnc process terminated unexpectedly."; exit 1; fi
            sleep 0.2
            ((count++))
            if [ $count -ge 50 ]; then echo "[ERROR] Timeout waiting for VNC server."; exit 1; fi
        done
        exec 3<&-
        exec 3>&-
        echo "VNC server is ready."
        /usr/bin/websockify -v --web /usr/share/novnc/ --idle-timeout=0 {instance.vnc_port} 127.0.0.1:$VNC_PORT &
        export DISPLAY=:{instance.vnc_display}
        xset s off -dpms
        xset s noblank
        /usr/bin/openbox-session &
        bash {dest_script_path} &
        wait
        """
        with open(vnc_launcher_path, 'w') as f:
            f.write(textwrap.dedent(vnc_launcher_content))
        os.chmod(vnc_launcher_path, stat.S_IRWXU)
        main_cmd = ['bash', vnc_launcher_path]

        proxy_target_port = instance.vnc_port
        nginx_conf_content = f"""
        location /ws/{instance.name}/ {{ proxy_pass http://127.0.0.1:{proxy_target_port}/; proxy_http_version 1.1; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection "upgrade"; }}
        location /app/{instance.name}/ {{
            if ($uri = /app/{instance.name}/) {{ return 302 $scheme://$http_host${{uri}}vnc.html?autoconnect=true&resize=remote&path=/ws/{instance.name}/; }}
            proxy_pass http://127.0.0.1:{proxy_target_port}/;
        }}"""
        with open(os.path.join(NGINX_LOCATIONS_D, f"{instance_slug}.conf"), 'w') as f:
            f.write(nginx_conf_content)
        _reload_nginx()
    else:
        # Standard mode now just runs the script directly. Socat is handled separately.
        main_cmd = ['bash', dest_script_path]

    output_log_path = os.path.join(instance_conf_dir, "output.log")
    with open(output_log_path, 'w') as output_log:
        main_process = subprocess.Popen(main_cmd, cwd=instance_conf_dir, env=env, stdout=output_log, stderr=output_log, preexec_fn=os.setsid)
    
    instance.pid = main_process.pid
    instance.status = "starting"
    db.commit()

    monitor = threading.Thread(
        target=monitor_instance_thread,
        args=(instance.id, instance.pid, internal_app_port, instance.vnc_display, instance_slug),
        daemon=True
    )
    monitor.start()

    running_instances[instance.id] = {
        "process": main_process, 
        "monitor_thread": monitor,
        "internal_port": internal_app_port,
        "activation_process": None # Initially not activated
    }
    print(f"[Manager] Started instance '{instance.name}' (PID: {instance.pid}) on internal port {internal_app_port}.")


def stop_instance_process(db: Session, instance: models.Instance):
    """
    Stops a running instance process, its monitor thread, and any active port forwarding.
    """
    instance_id = instance.id
    
    # Deactivate port forwarding first
    deactivate_instance_port(instance_id)
    
    if instance_id not in running_instances:
        print(f"[Manager] Stop requested, but instance {instance_id} not in running_instances dict. Cleaning up files.")
    else:
        process_info = running_instances[instance_id]
        process = process_info["process"]
        print(f"[Manager] Stopping process group for PID {process.pid}...")
        try:
            os.killpg(os.getpgid(process.pid), signal.SIGTERM)
            process.wait(timeout=10)
        except ProcessLookupError:
            print(f"[Manager] Process with PID {process.pid} not found.")
        except subprocess.TimeoutExpired:
            print(f"[Manager] Process did not terminate gracefully. Sending SIGKILL.")
            os.killpg(os.getpgid(process.pid), signal.SIGKILL)
        except Exception as e:
            print(f"[Manager] Error stopping process: {e}")
        
        del running_instances[instance_id]
    
    if instance.persistent_mode:
        _cleanup_instance_files(_slugify(instance.name))
    
    metadata = parse_blueprint_metadata(instance.base_blueprint)
    if metadata.get('app_id') == 'comfyui' and not instance.persistent_mode and instance.is_comfyui_active_slot:
        print(f"[Manager] Instance '{instance.name}' was the active ComfyUI slot. Triggering proxy deactivation.")
        update_comfyui_proxy(db)

    instance.status = "stopped"
    instance.pid = None
    db.commit()
    print(f"[Manager] Instance {instance.name} stopped and cleaned up.")